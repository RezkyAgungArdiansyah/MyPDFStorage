{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ecd109b-0702-400c-9fc0-30478c875600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdcd48a3-96de-4004-8fc8-5b23bd194e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6c0b0fb-5225-4096-bcc6-4a328a3d573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35177\n",
      "['volcanoes', 'glistening', 'parable', 'burp', 'interminably', 'moriarty', 'badlands', 'prevent', 'canine', 'flooding', 'juliette', 'disjointment', 'dire', 'sighing', 'leisure', 'hygienists', 'alum', 'hortense', 'negotiators', 'petstore', 'sleight', 'choked', 'raise', '1951', 'thrax', 'cynically', 'slobby', 'oranges', 'admire', 'modernizing', 'pacification', 'snoozing', 'happenings', 'braveheart', 'jazz', 'nipples', 'watts', 'lump', 'engaged', 'thunk', 'tumbling', 'spunky', 'climate', 'rapper', 'faceted', 'shaker', 'demonic', 'dexterity', 'irrefutable', 'lifeforms', 'int', 'perfect', 'katanga', 'crunchily', 'derailing', 'howled', 'varley', 'behold', 'cure', 'turns', 'suspenseless', 'uncontrollably', 'band', 'gunk', 'gumpian', 'vengeful', 'crud', 'directorial', 'bungalow', 'taunt', 'earning', 'doves', 'siouxsie', 'carousing', 'coping', 'caller', 'healed', 'engineers', 'slew', 'ac3', 'teaches', 'delegation', 'desk', 'phases', 'prevents', 'hell', 'mistaken', 'blowjobs', 'categorize', 'griswold', 'whew', 'merged', 'sectors', 'spy', 'lipnicki', 'reappears', 'collaborate', 'bah', 'joely', 'site']\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "feature_vector = count_vectorizer.fit(train_data['reviews_content'])\n",
    "features = count_vectorizer.get_feature_names_out()\n",
    "print(len(count_vectorizer.get_feature_names_out()))\n",
    "features = list(features)\n",
    "print(random.sample(features,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea127193-d8f1-44c9-b652-b34733577d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density of the matrix:  0.936727596630374\n"
     ]
    }
   ],
   "source": [
    "train_data_MatVec = count_vectorizer.transform(train_data['reviews_content'])\n",
    "train_data_MatVec.getnnz()\n",
    "print(\"Density of the matrix: \", train_data_MatVec.getnnz()*100/\n",
    "               (train_data_MatVec.shape[0]*train_data_MatVec.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31676509-ab65-4b34-b9b4-8ef452cb3795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>important</th>\n",
       "      <th>fix</th>\n",
       "      <th>it</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    important  fix  it\n",
       "0           1    0   8\n",
       "1           0    0  12\n",
       "2           1    0   2\n",
       "3           0    0  12\n",
       "4           0    0   7\n",
       "5           0    0   9\n",
       "6           0    0   9\n",
       "7           0    0   4\n",
       "8           0    0   3\n",
       "9           0    0   2\n",
       "10          0    0   5\n",
       "11          0    0   1\n",
       "12          0    0   7\n",
       "13          0    0   4\n",
       "14          0    0  40\n",
       "15          0    0  11\n",
       "16          0    0  11\n",
       "17          0    0   6\n",
       "18          0    0  13\n",
       "19          0    0  25"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_Vec_df = pd.DataFrame(train_data_MatVec.todense())\n",
    "train_data_Vec_df.columns = features\n",
    "train_data_Vec_df[['important','fix','it']][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb316b1e-b9e6-47b7-bc7b-3213c6d6fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_counts = np.sum(train_data_MatVec.toarray(),axis=0)\n",
    "features_counts_df = pd.DataFrame(dict(features = features, counts = features_counts))\n",
    "features_counts_df = features_counts_df.sort_values('counts',ascending=False)\n",
    "\n",
    "# features_counts_df.counts.sort_values(ascending= False)\n",
    "\n",
    "# x = features_counts_df.sort_values('counts',ascending=False)\n",
    "# for i in range(100):\n",
    "#     print(x.iloc[i])\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.hist(features_counts_df.counts)\n",
    "# plt.xlabel('Frequency of Words')\n",
    "# plt.ylabel('Density')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30415b48-42b4-4542-921a-620f69c76213",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_counts_df.to_csv('features_counts_df_2.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6126f861-beb6-4c83-9561-d91d91a51f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stop words:  frozenset({'find', 'five', 'here', 'please', 'on', 'top', 'thick', 'latter', 'into', 'your', 'out', 'indeed', 'under', 'before', 'formerly', 'cant', 'etc', 'other', 'alone', 'fire', 'should', 'beside', 'one', 'beforehand', 'she', 'have', 'thru', 'six', 'bottom', 'was', 'such', 'too', 'for', 'less', 'her', 'as', 'latterly', 'itself', 'off', 'onto', 'beyond', 'get', 'amongst', 'anywhere', 'can', 'thereby', 'herein', 'mine', 'found', 'anything', 'therein', 'therefore', 'sometimes', 'eleven', 'is', 'wherein', 'hundred', 'he', 'made', 'hereby', 'co', 'they', 'also', 'from', 'among', 'upon', 'amount', 'us', 'empty', 'mostly', 'their', 'then', 'what', 'ever', 'were', 'whether', 'wherever', 'when', 'due', 'much', 'give', 'ltd', 'already', 'nor', 'every', 'seeming', 'call', 'however', 'full', 'behind', 'even', 'some', 're', 'toward', 'those', 'his', 'forty', 'over', 'namely', 'everything', 'describe', 'because', 'keep', 'becoming', 'now', 'would', 'con', 'without', 'whenever', 'why', 'against', 'myself', 'there', 'whereupon', 'him', 'up', 'could', 'if', 'you', 'most', 'the', 'bill', 'yours', 'them', 'together', 'almost', 'meanwhile', 'both', 'either', 'inc', 'we', 'amoungst', 'herself', 'these', 'perhaps', 'am', 'had', 'it', 'others', 'moreover', 'ourselves', 'how', 'next', 'mill', 'become', 'else', 'becomes', 'to', 'above', 'being', 'couldnt', 'nothing', 'system', 'thereupon', 'thus', 'twenty', 'whither', 'me', 'may', 'seem', 'must', 'that', 'down', 'with', 'name', 'will', 'back', 'whole', 'everyone', 'whose', 'all', 'show', 'thence', 'front', 'throughout', 'hereupon', 'its', 'part', 'through', 'a', 'own', 'this', 'another', 'after', 'fifteen', 'nine', 'interest', 'move', 'none', 'might', 'in', 'again', 'two', 'during', 'although', 'further', 'by', 'any', 'three', 'something', 'each', 'who', 'whence', 'than', 'nobody', 'rather', 'un', 'where', 'take', 'anyone', 'via', 'along', 'last', 'not', 'seemed', 'eight', 'afterwards', 'ie', 'around', 'an', 'only', 'third', 'side', 'been', 'became', 'fill', 'across', 'whom', 'still', 'of', 'several', 'since', 'first', 'themselves', 'once', 'cry', 'within', 'always', 'sixty', 'so', 'between', 'eg', 'my', 'neither', 'few', 'go', 'well', 'no', 'someone', 'below', 'cannot', 'sometime', 'are', 'yourself', 'though', 'whereafter', 'until', 'more', 'see', 'former', 'nowhere', 'elsewhere', 'anyway', 'done', 'de', 'be', 'but', 'noone', 'fifty', 'everywhere', 'our', 'sincere', 'ours', 'serious', 'hereafter', 'which', 'thin', 'or', 'often', 'same', 'per', 'has', 'very', 'whereas', 'detail', 'do', 'himself', 'thereafter', 'least', 'i', 'whatever', 'hasnt', 'nevertheless', 'towards', 'yourselves', 'except', 'hers', 'ten', 'twelve', 'otherwise', 'besides', 'hence', 'at', 'many', 'yet', 'enough', 'and', 'put', 'whoever', 'somehow', 'somewhere', 'never', 'anyhow', 'whereby', 'about', 'four', 'seems', 'while'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "print(\"List of stop words: \", (my_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bad9c7a-771c-4dce-a2f2-d534c2837227",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = set(my_stop_words)\n",
    "excluded_words = {'not','nor','cannot','neither','more','whereas','mostly','very'}\n",
    "my_stop_words = my_stop_words - excluded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd828b-6a4d-4290-97ce-67d69796425f",
   "metadata": {},
   "source": [
    "# Reprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94e205b3-8dc4-449f-8ed6-d7c539b86947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test  = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "352da885-85ad-4f90-b3da-555b7dfa1eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>airplane ! is considered among many to be the ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you've got to love disney . \\nno matter what t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" the tailor of panama \" is a different kind ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the characters in jonathan lynn's \" the whole ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vikings v . bears ? \\nno , this isn't the line...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>trekkies , roger nygard's energetic and hilari...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>\" dangerous beauty \" is a really nothing more...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>starring shawnee smith ; donovan leitch ; rick...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>man , this was one wierd movie . \\nsimilar to ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>review : ghost dog : the way of the samurai ( ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        reviews_content  category\n",
       "0     airplane ! is considered among many to be the ...  positive\n",
       "1     you've got to love disney . \\nno matter what t...  positive\n",
       "2      \" the tailor of panama \" is a different kind ...  positive\n",
       "3     the characters in jonathan lynn's \" the whole ...  negative\n",
       "4     vikings v . bears ? \\nno , this isn't the line...  negative\n",
       "...                                                 ...       ...\n",
       "1495  trekkies , roger nygard's energetic and hilari...  positive\n",
       "1496   \" dangerous beauty \" is a really nothing more...  positive\n",
       "1497  starring shawnee smith ; donovan leitch ; rick...  negative\n",
       "1498  man , this was one wierd movie . \\nsimilar to ...  negative\n",
       "1499  review : ghost dog : the way of the samurai ( ...  positive\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab61c7c2-ff3e-48e1-8399-07abc95e51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in words if word.lower() not in my_stop_words]\n",
    "    # Join the remaining words back into a single string\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    return processed_text\n",
    "def remove_line_break(text):\n",
    "    words = text.split()\n",
    "    output = [element.replace(\"\\n\",\"\") for element in words]\n",
    "    output = ' '.join(output)\n",
    "    return output\n",
    "def extract_alphabetic(text):\n",
    "    # Use regular expression to remove non-alphabetic characters\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "df_train['Processed Contents'] = df_test['reviews_content'].apply(lambda x: [element.replace(\"\\n\", \"\") for element in x.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5d21cf1-762a-414d-87d6-3e3698c6adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Processed Contents'] = df_train['reviews_content'].apply(remove_line_break)\n",
    "\n",
    "df_train['Processed Contents'] = df_train['Processed Contents'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "830de6d7-d7e5-4f7c-8a7d-f77aa5a8c5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_content</th>\n",
       "      <th>category</th>\n",
       "      <th>Processed Contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>airplane ! is considered among many to be the ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>airplane ! considered epitome satire film-maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you've got to love disney . \\nno matter what t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>you've got love disney . matter serve , guaran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" the tailor of panama \" is a different kind ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>\" tailor panama \" different kind spy movie . d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the characters in jonathan lynn's \" the whole ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>characters jonathan lynn's \" yards , \" endless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vikings v . bears ? \\nno , this isn't the line...</td>\n",
       "      <td>negative</td>\n",
       "      <td>vikings v . bears ? , isn't lineup monday nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>trekkies , roger nygard's energetic and hilari...</td>\n",
       "      <td>positive</td>\n",
       "      <td>trekkies , roger nygard's energetic hilarious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>\" dangerous beauty \" is a really nothing more...</td>\n",
       "      <td>positive</td>\n",
       "      <td>\" dangerous beauty \" really more grandiose soa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>starring shawnee smith ; donovan leitch ; rick...</td>\n",
       "      <td>negative</td>\n",
       "      <td>starring shawnee smith ; donovan leitch ; rick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>man , this was one wierd movie . \\nsimilar to ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>man , wierd movie . similar conspiracy theory ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>review : ghost dog : the way of the samurai ( ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>review : ghost dog : way samurai ( 1999 ) cast...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        reviews_content  category  \\\n",
       "0     airplane ! is considered among many to be the ...  positive   \n",
       "1     you've got to love disney . \\nno matter what t...  positive   \n",
       "2      \" the tailor of panama \" is a different kind ...  positive   \n",
       "3     the characters in jonathan lynn's \" the whole ...  negative   \n",
       "4     vikings v . bears ? \\nno , this isn't the line...  negative   \n",
       "...                                                 ...       ...   \n",
       "1495  trekkies , roger nygard's energetic and hilari...  positive   \n",
       "1496   \" dangerous beauty \" is a really nothing more...  positive   \n",
       "1497  starring shawnee smith ; donovan leitch ; rick...  negative   \n",
       "1498  man , this was one wierd movie . \\nsimilar to ...  negative   \n",
       "1499  review : ghost dog : the way of the samurai ( ...  positive   \n",
       "\n",
       "                                     Processed Contents  \n",
       "0     airplane ! considered epitome satire film-maki...  \n",
       "1     you've got love disney . matter serve , guaran...  \n",
       "2     \" tailor panama \" different kind spy movie . d...  \n",
       "3     characters jonathan lynn's \" yards , \" endless...  \n",
       "4     vikings v . bears ? , isn't lineup monday nigh...  \n",
       "...                                                 ...  \n",
       "1495  trekkies , roger nygard's energetic hilarious ...  \n",
       "1496  \" dangerous beauty \" really more grandiose soa...  \n",
       "1497  starring shawnee smith ; donovan leitch ; rick...  \n",
       "1498  man , wierd movie . similar conspiracy theory ...  \n",
       "1499  review : ghost dog : way samurai ( 1999 ) cast...  \n",
       "\n",
       "[1500 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8c52e52-41cf-4e53-81fd-63b08a935078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1d47285b-cc7b-4898-8e07-112de5b1a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_Vectorizer = TfidfVectorizer()\n",
    "TFIDF_Vectorizer.fit(df_train['reviews_content'])\n",
    "X_train = TFIDF_Vectorizer.transform(df_train['reviews_content'])\n",
    "y_train = df_train['category']\n",
    "df_test = pd.read_csv('test.csv')\n",
    "X_test = TFIDF_Vectorizer.transform(df_test['reviews_content'])\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, df_train['category'], test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57946931-bba2-4a9d-a8b7-952d11120905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 35177)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model  = SVC(kernel = 'linear')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d93c15c1-0822-4124-94b4-e47ad48b5bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error : 0.9874074074074074\n",
      "val error : 0.8266666666666667\n"
     ]
    }
   ],
   "source": [
    "svm_model.fit(X_train,y_train)\n",
    "y_pred_train = svm_model.predict(X_train)\n",
    "y_pred_val = svm_model.predict(X_val)\n",
    "print(\"train error :\",accuracy_score(y_train,y_pred_train))\n",
    "print(\"val error :\",accuracy_score(y_val,y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "11124cef-b83e-40ec-b693-22b3a89648a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40463\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "#This code concatenates all the text from the reviews_content column in the DataFrame df into a single string, with each review separated by a space.\n",
    "reviews = df_train.reviews_content.str.cat(sep=' ')\n",
    "tokens = word_tokenize(reviews)\n",
    "vocabulary = set(tokens)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98e98e04-d072-43fa-a2a4-2a686ef34770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "frecu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2ae7f725-780c-46ea-bef1-a535e601d1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error : 0.968\n",
      "val error : 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_val = clf.predict(X_val)\n",
    "print(\"train error :\",accuracy_score(y_train,y_pred_train))\n",
    "print(\"val error :\",accuracy_score(y_val,y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "76277d79-b1b3-486e-af22-13720a43e2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArrowDtype', 'BooleanDtype', 'Categorical', 'CategoricalDtype', 'CategoricalIndex', 'DataFrame', 'DateOffset', 'DatetimeIndex', 'DatetimeTZDtype', 'ExcelFile', 'ExcelWriter', 'Flags', 'Float32Dtype', 'Float64Dtype', 'Grouper', 'HDFStore', 'Index', 'IndexSlice', 'Int16Dtype', 'Int32Dtype', 'Int64Dtype', 'Int8Dtype', 'Interval', 'IntervalDtype', 'IntervalIndex', 'MultiIndex', 'NA', 'NaT', 'NamedAgg', 'Period', 'PeriodDtype', 'PeriodIndex', 'RangeIndex', 'Series', 'SparseDtype', 'StringDtype', 'Timedelta', 'TimedeltaIndex', 'Timestamp', 'UInt16Dtype', 'UInt32Dtype', 'UInt64Dtype', 'UInt8Dtype', '__all__', '__builtins__', '__cached__', '__doc__', '__docformat__', '__file__', '__git_version__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_built_with_meson', '_config', '_is_numpy_dev', '_libs', '_pandas_datetime_CAPI', '_pandas_parser_CAPI', '_testing', '_typing', '_version_meson', 'annotations', 'api', 'array', 'arrays', 'bdate_range', 'compat', 'concat', 'core', 'crosstab', 'cut', 'date_range', 'describe_option', 'errors', 'eval', 'factorize', 'from_dummies', 'get_dummies', 'get_option', 'infer_freq', 'interval_range', 'io', 'isna', 'isnull', 'json_normalize', 'lreshape', 'melt', 'merge', 'merge_asof', 'merge_ordered', 'notna', 'notnull', 'offsets', 'option_context', 'options', 'pandas', 'period_range', 'pivot', 'pivot_table', 'plotting', 'qcut', 'read_clipboard', 'read_csv', 'read_excel', 'read_feather', 'read_fwf', 'read_gbq', 'read_hdf', 'read_html', 'read_json', 'read_orc', 'read_parquet', 'read_pickle', 'read_sas', 'read_spss', 'read_sql', 'read_sql_query', 'read_sql_table', 'read_stata', 'read_table', 'read_xml', 'reset_option', 'set_eng_float_format', 'set_option', 'show_versions', 'test', 'testing', 'timedelta_range', 'to_datetime', 'to_numeric', 'to_pickle', 'to_timedelta', 'tseries', 'unique', 'util', 'value_counts', 'wide_to_long']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "df_output = pd.DataFrame({'Row': range(1,501),'Label':y_pred_test})\n",
    "print(dir(pd))\n",
    "df_output.to_csv('MultinomialNaiveBayes.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c9225fdf-9c7b-4ce1-b2d4-6f47a936909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error : 0.9992592592592593\n",
      "val error : 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Cross Valdation\n",
    "model_gaussianNB = GaussianNB()\n",
    "model_gaussianNB.fit(X_train.toarray(),y_train)\n",
    "y_pred_val = model_gaussianNB.predict(X_val.toarray())\n",
    "y_pred_train = model_gaussianNB.predict(X_train.toarray())\n",
    "print(\"train error :\",accuracy_score(y_train,y_pred_train))\n",
    "print(\"val error :\",accuracy_score(y_val,y_pred_val))\n",
    "# real\n",
    "model_gaussianNB = GaussianNB()\n",
    "model_gaussianNB.fit(X_train.toarray(),y_train)\n",
    "y_pred_test = model_gaussianNB.predict(X_test.toarray())\n",
    "df_output = pd.DataFrame({'Row': range(1,501),'Label':y_pred_test})\n",
    "df_output.to_csv('GaussianNaiveBayes.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
