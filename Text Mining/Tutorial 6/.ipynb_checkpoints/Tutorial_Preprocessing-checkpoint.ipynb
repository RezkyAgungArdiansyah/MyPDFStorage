{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8228de",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf7b78",
   "metadata": {},
   "source": [
    "Some of the common text preprocessing / cleaning steps are:\n",
    "\n",
    "1. Lower casing\n",
    "2. Removal of Punctuations\n",
    "3. Removal of Stopwords\n",
    "4. Removal of Frequent words\n",
    "5. Removal of Rare words\n",
    "6. Stemming, Lemmatization\n",
    "7. Removal of emojis\n",
    "8. Removal of emoticons\n",
    "9. Conversion of emoticons to words\n",
    "10. Conversion of emojis to words\n",
    "11. Removal of URLs\n",
    "12. Removal of HTML tags\n",
    "13. Chat words conversion\n",
    "14. Spelling correction.\n",
    "\n",
    "So these are the different types of text preprocessing steps we can do on text data. But we need not do all of these all the time. We must carefully choose the preprocessing steps based on our use case since that also plays an important role.\n",
    "\n",
    "For example, in the sentiment analysis use case, we need not remove the emojis or emoticons as they will convey important information about the sentiment. Similarly, we need to decide based on our use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed12d7a",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "full_df = pd.read_csv(\"sample.csv\", nrows=5000)\n",
    "df = full_df[[\"text\"]]\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823e982",
   "metadata": {},
   "source": [
    "# Lower Casing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbf21c",
   "metadata": {},
   "source": [
    "Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way.\n",
    "\n",
    "This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.\n",
    "\n",
    "This may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)\n",
    "\n",
    "By default, lower casing is done my most of the modern day vecotirzers and tokenizers like sklearn TfidfVectorizer and Keras Tokenizer. So we need to set them to false as needed depending on our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_lower\"] = df[\"text\"].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737a9b7",
   "metadata": {},
   "source": [
    "# Punctuations Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033716d7",
   "metadata": {},
   "source": [
    "One another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n",
    "\n",
    "We also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the string. Punctuation in Python contains the following punctuation symbols\n",
    "\n",
    "!\"#$%&'()*+,-./:;<=>?@[\\]^_{|}~`\n",
    "\n",
    "We can add or remove more punctuation as per our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ccb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the new column created in last cell\n",
    "df.drop([\"text_lower\"], axis=1, inplace=True)\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "df[\"text_wo_punct\"] = df[\"text\"].apply(lambda text: remove_punctuation(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdeab00",
   "metadata": {},
   "source": [
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2b783",
   "metadata": {},
   "source": [
    "Stopwords are commonly occurring words in a language like 'the', 'a', and so on. They can be removed from the text most of the time, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\n",
    "\n",
    "These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for English language from the NLTK package can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec10a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e256fa",
   "metadata": {},
   "source": [
    "Similarly we can also get the list for other languages as well and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59174815",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "df[\"text_wo_stop\"] = df[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c2a4b",
   "metadata": {},
   "source": [
    "# Frequent Words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9e68e",
   "metadata": {},
   "source": [
    "In the previous preprocessing step, we removed the stopwords based on language information. But say, if we have a domain-specific corpus, we might also have some frequent words that are of not so much importance to us.\n",
    "\n",
    "So this step is to remove the frequent words in the given corpus. If we use something like TF*IDF, this is automatically taken care of.\n",
    "\n",
    "Let us get the most common words and then remove them in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0973c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"text_wo_stop\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "df[\"text_wo_stopfreq\"] = df[\"text_wo_stop\"].apply(lambda text: remove_freqwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b045c67",
   "metadata": {},
   "source": [
    "# Rare Words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edfd9a",
   "metadata": {},
   "source": [
    "This is very similar to previous preprocessing step but we will remove the rare words from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the two columns which are no more needed \n",
    "df.drop([\"text_wo_punct\", \"text_wo_stop\"], axis=1, inplace=True)\n",
    "\n",
    "n_rare_words = 10\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "df[\"text_wo_stopfreqrare\"] = df[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a37bb0",
   "metadata": {},
   "source": [
    "We can combine all the lists of words (stopwords, frequent words, and rare words) and create a single list to remove them at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631fb034",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6d1aa",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base, or root form (From Wikipedia)\n",
    "\n",
    "For example, if there are two words in the corpus walks and walking, then stemming will stem the suffix to make them walk. But say in another example, we have two words console and consoling, the stemmer will remove the suffix and make them consol which is not a proper English word.\n",
    "\n",
    "There are several types of stemming algorithms available and one of the famous ones is the Porter Stemmer which is widely used. We can use the NLTK package for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834dc04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Drop the two columns \n",
    "df.drop([\"text_wo_stopfreq\", \"text_wo_stopfreqrare\"], axis=1, inplace=True) \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "df[\"text_stemmed\"] = df[\"text\"].apply(lambda text: stem_words(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1607ebb",
   "metadata": {},
   "source": [
    "We can see that words like private and propose to have their e at the end chopped off due to stemming. This is not intended. What can we do about that? We can use Lemmatization in such cases.\n",
    "\n",
    "Also, this Porter Stemmer is for the English language. If we are working with other languages, we can use snowball stemmer. The supported languages for snowball stemmer are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f399563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3f372",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3dce03",
   "metadata": {},
   "source": [
    "Lemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language.\n",
    "\n",
    "As a result, this one is generally slower than the stemming process. So depending on the speed requirement, we can choose to use either stemming or lemmatization.\n",
    "\n",
    "Let us use the WordNetLemmatizer in NLTK to lemmatize our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea0d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df[\"text_lemmatized\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee9e62",
   "metadata": {},
   "source": [
    "We can see that the trailing e in the propose and private is retained when we use lemmatization unlike stemming.\n",
    "\n",
    "Wait. There is one more thing in lemmatization. Let us try to lemmatize running now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db582d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7029e8e",
   "metadata": {},
   "source": [
    "Wow. It returned running as such without converting it to the root form run. This is because the lemmatization process depends on the POS tag to come up with the correct lemma. Now let us lemmatize again by providing the POS tag for the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"running\", \"v\") # v for verb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e081e",
   "metadata": {},
   "source": [
    "Now we are getting the root form run. So we also need to provide the POS tag of the word along with the word for lemmatizer in NLTK. Depending on the POS, the lemmatizer may return different results.\n",
    "\n",
    "Let us take the example, stripes and check the lemma when it is both verb and noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word is : stripes\")\n",
    "print(\"Lemma result for verb : \",lemmatizer.lemmatize(\"stripes\", 'v'))\n",
    "print(\"Lemma result for noun : \",lemmatizer.lemmatize(\"stripes\", 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395e74f",
   "metadata": {},
   "source": [
    "Now let us redo the lemmatization process for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "df[\"text_lemmatized\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5df34c",
   "metadata": {},
   "source": [
    "We can now see that in the third row, sent got converted to send since we provided the POS tag for lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6b004",
   "metadata": {},
   "source": [
    "# Emoji Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709e87b",
   "metadata": {},
   "source": [
    "With more and more usage of social media platforms, there is an explosion in the usage of emojis in our day to day life as well. Probably we might need to remove these emojis for some of our textual analysis.\n",
    "\n",
    "Thanks to [this code](https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b), please find below a helper function to remove emojis from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdaf1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "remove_emoji(\"game is on 🔥🔥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_emoji(\"Hilarious😂\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa7ed0",
   "metadata": {},
   "source": [
    "# Emoticons Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ddf69",
   "metadata": {},
   "source": [
    "This is what we did in the last step right? Nope. We did remove emojis in the last step but not emoticons. There is a minor difference between emojis and emoticons.\n",
    "\n",
    "According to Grammarist.com, the emoticon is built from keyboard characters that when put together in a certain way represent a facial expression, an emoji is an actual image.\n",
    "\n",
    ":-) is an emoticon\n",
    "\n",
    "😀 is an emoji\n",
    "\n",
    "Thanks to [NeelShah](https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py) for the wonderful collection of emoticons, we are going to use them to remove emoticons.\n",
    "\n",
    "Please note again that the removal of emojis/emoticons is not always preferred and decisions should be made based on the use case at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385afcfc",
   "metadata": {},
   "source": [
    "NOTE: Please copy and paste the code from EMOTICONS.txt here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0760a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoticons(\"Hello :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd079f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_emoticons(\"I am sad :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d056ab",
   "metadata": {},
   "source": [
    "# Convert Emoticon to Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6399bef",
   "metadata": {},
   "source": [
    "In the previous step, we have removed the emoticons. In case of use cases like sentiment analysis, the emoticons give some valuable information so removing them might not be a good solution. What can we do in such cases?\n",
    "\n",
    "One way is to convert the emoticons to word format so that they can be used in downstream modeling processes. Thanks to Neel again for the wonderful dictionary that we used in the previous step. We are going to use that again for the conversion of emoticons to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75066ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am sad :()\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003975ce",
   "metadata": {},
   "source": [
    "This method might be better for some use cases when we do not want to miss out on the emoticon information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1177edab",
   "metadata": {},
   "source": [
    "# Convert Emoji to Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a428485",
   "metadata": {},
   "source": [
    "Now let us do the same for Emojis as well. Neel Shah has put together a list of emojis with the corresponding words as well as part of his [Github repo](https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py). We are going to make use of this dictionary to convert the emojis to corresponding words.\n",
    "\n",
    "Again this conversion might be better than emoji removal for certain use cases. Please use the one that is suitable for the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cbbbe5",
   "metadata": {},
   "source": [
    "NOTE: Please copy and paste the code from EMO_UNICODE.txt here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78faec81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "text = \"game is on 🔥\"\n",
    "convert_emojis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hilarious 😂\"\n",
    "convert_emojis(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a70092",
   "metadata": {},
   "source": [
    "# URL Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657bb2cf",
   "metadata": {},
   "source": [
    "The next preprocessing step is to remove any URLs present in the data. For example, if we are doing a Twitter analysis, then there is a good chance that the tweet will have some URL in it. Probably we might need to remove them for our further analysis.\n",
    "\n",
    "We can use the below code snippet to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b08ea",
   "metadata": {},
   "source": [
    "Let us take a https link and check the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Driverless AI NLP blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3310a",
   "metadata": {},
   "source": [
    "Now let us take a http url and check the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd2976",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Please refer to link http://lnkd.in/ecnt5yC for the paper\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8fcd3",
   "metadata": {},
   "source": [
    "Thanks to Pranjal for the edge cases in the comments below. Suppose say there is no http or https in the url link. The function can now captures that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d1cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Want to know more. Checkout www.h2o.ai for additional information\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef93beb",
   "metadata": {},
   "source": [
    "# HTML Tags Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2137e19c",
   "metadata": {},
   "source": [
    "One another common preprocessing technique that will come in handy in multiple places is the removal of HTML tags. This is especially useful if we scrap the data from different websites. We might end up having HTML strings as part of our text.\n",
    "\n",
    "First, let us try to remove the HTML tags using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "text = \"\"\"<div>\n",
    "<h1> H2O</h1>\n",
    "<p> AutoML</p>\n",
    "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
    "</div>\"\"\"\n",
    "\n",
    "print(remove_html(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f6134",
   "metadata": {},
   "source": [
    "We can also use BeautifulSoup package to get the text from HTML document in a more elegant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79498bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").text\n",
    "    #return BeautifulSoup(text, \"lxml\").text --> you also can use this code\n",
    "\n",
    "s = \"\"\"<div>\n",
    "<h1> H2O</h1>\n",
    "<p> AutoML</p>\n",
    "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "print(remove_html(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c665b6a6",
   "metadata": {},
   "source": [
    "# Chat Words Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b46426",
   "metadata": {},
   "source": [
    "This is an important text preprocessing step if we are dealing with chat data. People do use a lot of abbreviated words in chat and so it might be helpful to expand those words for our analysis purposes.\n",
    "\n",
    "Got a good list of chat slang words from this [repo](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt). We can use this for our conversion here. We can add more words to this list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5e337",
   "metadata": {},
   "source": [
    "NOTE: Please copy and paste the code from chat_words_str.txt here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017fe97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3226a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in chat_words_str.split(\"\\n\"):\n",
    "    if line != \"\":\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(chat_words_map_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "chat_words_conversion(\"one minute BRB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_conversion(\"imo this is awesome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac397a5d",
   "metadata": {},
   "source": [
    "We can add more words to our abbreviation list and use them based on our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f624822",
   "metadata": {},
   "source": [
    "# Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc039c8",
   "metadata": {},
   "source": [
    "One another important text preprocessing step is spelling correction. Typos are common in text data and we might want to correct those spelling mistakes before we do our analysis.\n",
    "\n",
    "If we are interested in wrinting a spell corrector of our own, we can probably start with [famous code](https://norvig.com/spell-correct.html) from Peter Norvig.\n",
    "\n",
    "In this notebook, let us use the python package pyspellchecker for our spelling correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a51c845",
   "metadata": {},
   "source": [
    "#pyspellchecker installation\n",
    "-->!conda install -n environment pyspellchecker -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c71d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "text = \"speling correctin\"\n",
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"thnks for readin the notebook\"\n",
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a08ee",
   "metadata": {},
   "source": [
    "# Tutorial Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e770fc2c",
   "metadata": {},
   "source": [
    "1. Find any textual dataset (not too big).\n",
    "2. Applied text preprocessing on the data, and choose an appropriate method for your data.\n",
    "3. Write a report containing: the reason for the selected method (why you choose those methods?), code implementation, and result of the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe94215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ghaluh",
   "language": "python",
   "name": "ghaluh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
