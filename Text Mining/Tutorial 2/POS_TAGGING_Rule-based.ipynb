{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b1a92d",
   "metadata": {},
   "source": [
    "# Rule-based approach Part-of-Speech-Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e8256",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8eb5694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:03.604230Z",
     "start_time": "2024-03-04T05:47:03.602013Z"
    }
   },
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca827103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:05.865303Z",
     "start_time": "2024-03-04T05:47:05.839285Z"
    }
   },
   "source": [
    "train = pd.read_csv(\"train_postag.csv\")\n",
    "test = pd.read_csv(\"test_postag.csv\")\n",
    "\n",
    "train[\"isTrain\"] = True\n",
    "test[\"isTrain\"] = False\n",
    "\n",
    "full = pd.concat([train, test])\n",
    "full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05311e1",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905a142d",
   "metadata": {},
   "source": [
    "The number of links and tags (like @Someone) seem to be important factors, as we tend to share links to the actual news when talking about disasters. Hashtags also seem to be important, along with the their number on a specific tweet, so we'll track those as well."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef1e9644",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:09.530926Z",
     "start_time": "2024-03-04T05:47:09.207532Z"
    }
   },
   "source": [
    "def get_at(row):\n",
    "    return re.findall(\"@[\\w]+\", row[\"text\"])\n",
    "\n",
    "def get_http(row):\n",
    "    return re.findall(\"http[\\:\\/\\.\\w]+\", row[\"text\"])\n",
    "\n",
    "def get_hashtags(row):\n",
    "    return re.findall(\"#[\\w]+\", row[\"text\"])\n",
    "\n",
    "def number_of_tags(row):\n",
    "    return len(row[\"tags\"])\n",
    "\n",
    "def number_of_links(row):\n",
    "    return len(row[\"links\"])\n",
    "\n",
    "def number_of_hashs(row):\n",
    "    return len(row[\"hashtags\"])\n",
    "\n",
    "def clean_text(row):\n",
    "    clean = row[\"text\"]\n",
    "    \n",
    "    if len(row[\"tags\"]) != 0:\n",
    "        for word in row[\"tags\"]:\n",
    "            clean = clean.replace(word, \"\")\n",
    "    \n",
    "    if len(row[\"links\"]) != 0:\n",
    "        for word in row[\"links\"]:\n",
    "            clean = clean.replace(word, \"\")\n",
    "    \n",
    "    #only remove the # symbol\n",
    "    clean = clean.replace(\"#\", \"\").replace(\"/\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    return clean.strip()\n",
    "\n",
    "full[\"tags\"] = full.apply(lambda row: get_at(row), axis = 1)\n",
    "full[\"links\"] = full.apply(lambda row: get_http(row), axis = 1)\n",
    "full[\"hashtags\"] = full.apply(lambda row: get_hashtags(row), axis = 1)\n",
    "\n",
    "full[\"number_of_tags\"] = full.apply(lambda row: number_of_tags(row), axis = 1)\n",
    "full[\"number_of_links\"] = full.apply(lambda row: number_of_links(row), axis = 1)\n",
    "full[\"number_of_hashs\"] = full.apply(lambda row: number_of_hashs(row), axis = 1)\n",
    "\n",
    "full[\"clean_text\"] = full.apply(lambda row: clean_text(row), axis = 1)\n",
    "full.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231878b",
   "metadata": {},
   "source": [
    "We have cleaned our texts and stored links, hashtags and tags, it's time for the real deal. We'll first tokenize our texts and use them to get our grammatical classes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f31acc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:15.781652Z",
     "start_time": "2024-03-04T05:47:14.782978Z"
    }
   },
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_tokens(row):\n",
    "    return word_tokenize(row[\"clean_text\"].lower())\n",
    "\n",
    "full[\"tokens\"] = full.apply(lambda row: get_tokens(row), axis = 1)\n",
    "full.sample(5, random_state = 4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fcf66630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:22.461939Z",
     "start_time": "2024-03-04T05:47:18.415436Z"
    }
   },
   "source": [
    "s = [\"screams\", \"in\", \"the\", \"distance\"]\n",
    "\n",
    "def get_postags(row):\n",
    "    \n",
    "    postags = nltk.pos_tag(row[\"tokens\"])\n",
    "    list_classes = list()\n",
    "    for  word in postags:\n",
    "        list_classes.append(word[1])\n",
    "    \n",
    "    return list_classes\n",
    "\n",
    "full[\"postags\"] = full.apply(lambda row: get_postags(row), axis = 1)\n",
    "full.sample(5, random_state = 4)\n",
    "# nltk.help.upenn_tagset('NNS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80fd96",
   "metadata": {},
   "source": [
    "Now we have the POS tags for every text. There are lots of categories and I'll focus only in a few of them. Here are the meanings:\n",
    "\n",
    "NN: noun (there are other categories that can fit within this one for our purposes, such as NNS, NNP, NNPS, which all belong to nouns, containing plurals and proper names)\n",
    "RB: adverb\n",
    "VB: verb (and similar categories indicating tense: VBP, VBG, VBS..)\n",
    "JJ: adjective or numeral\n",
    "Note: If you want to get more information about the classes that pos_tag can identify, use the command nltk.help.upenn_tagset(\"NNS\") for instance. If you want to see all the similar categories, you can just write the first letter of the class: nltk.help.upenn_tagset(\"N\")."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4835db99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:27.168435Z",
     "start_time": "2024-03-04T05:47:26.847805Z"
    }
   },
   "source": [
    "def find_no_class(count, class_name = \"\"):\n",
    "    total = 0\n",
    "    for key in count.keys():\n",
    "        if key.startswith(class_name):\n",
    "            total += count[key]\n",
    "            \n",
    "            \n",
    "    return total\n",
    "\n",
    "def get_classes(row, grammatical_class = \"\"):\n",
    "    count = Counter(row[\"postags\"])\n",
    "    return find_no_class(count, class_name = grammatical_class)/len(row[\"postags\"])\n",
    "\n",
    "full[\"freqAdverbs\"] = full.apply(lambda row: get_classes(row, \"RB\"), axis = 1)\n",
    "full[\"freqVerbs\"] = full.apply(lambda row: get_classes(row, \"VB\"), axis = 1)\n",
    "full[\"freqAdjectives\"] = full.apply(lambda row: get_classes(row, \"JJ\"), axis = 1)\n",
    "full[\"freqNouns\"] = full.apply(lambda row: get_classes(row, \"NN\"), axis = 1)\n",
    "\n",
    "full.sample(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5562ae53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:36.630069Z",
     "start_time": "2024-03-04T05:47:36.622725Z"
    }
   },
   "source": [
    "training = full.loc[full[\"isTrain\"] == True, :].copy()\n",
    "testing = full.loc[full[\"isTrain\"] == False, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fa2e0",
   "metadata": {},
   "source": [
    "# A few visualizations of the POS for disaster and non-disaster tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6ca23",
   "metadata": {},
   "source": [
    "Note: non-disaster tweets are shown in blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfee113",
   "metadata": {},
   "source": [
    "Noun frequency"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1037589b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:38.844222Z",
     "start_time": "2024-03-04T05:47:38.743205Z"
    }
   },
   "source": [
    "training.loc[training[\"target\"] == 0.0, \"freqNouns\"].hist(alpha = 0.5);\n",
    "training.loc[training[\"target\"] == 1.0, \"freqNouns\"].hist(alpha = 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c23ac2",
   "metadata": {},
   "source": [
    "Verb Frequency"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b1f8cff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:42.095979Z",
     "start_time": "2024-03-04T05:47:41.998915Z"
    }
   },
   "source": [
    "training.loc[training[\"target\"] == 0.0, \"freqVerbs\"].hist(alpha = 0.5);\n",
    "training.loc[training[\"target\"] == 1.0, \"freqVerbs\"].hist(alpha = 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919694d9",
   "metadata": {},
   "source": [
    "Adjective frequency"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7928a47f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:44.550218Z",
     "start_time": "2024-03-04T05:47:44.446745Z"
    }
   },
   "source": [
    "training.loc[training[\"target\"] == 0.0, \"freqAdjectives\"].hist(alpha = 0.5);\n",
    "training.loc[training[\"target\"] == 1.0, \"freqAdjectives\"].hist(alpha = 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d23cd",
   "metadata": {},
   "source": [
    "The distribution of frequencies for the three grammatical classes are about the same for disaster and non-disaster tweets. The most prominent difference being the number of instances. In that case, we have more non-disaster than disaster tweets in the training dataset. This distribution study is very important to do because there are cases (such as fake news detection mentioned earlier) in which both distributions have statistically different distributions, and this fact may help in detection.\n",
    "\n",
    "We are ready to make a first model out of the variables we have. Let's create a simple train-test split."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0546d844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:48.704025Z",
     "start_time": "2024-03-04T05:47:46.927135Z"
    }
   },
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "x = training.loc[:, [\"number_of_tags\", \"number_of_links\", \"freqAdverbs\", \"freqVerbs\", \"freqAdjectives\", \"freqNouns\"]]\n",
    "y = training.loc[:, \"target\"]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(x, y)\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = x.loc[train_index], x.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "    clf = GradientBoostingClassifier(learning_rate=0.1, max_depth= 5, max_features = 5,random_state = 42)\n",
    "#     clf = RandomForestClassifier(random_state = 42)\n",
    "    \n",
    "    clf.fit(x_train, y_train)\n",
    "    preds = clf.predict(x_test)\n",
    "    \n",
    "    print(accuracy_score(y_test, preds))\n",
    "    \n",
    "    print(confusion_matrix(y_test, preds))\n",
    "\n",
    "\n",
    "total_preds = clf.predict(x)\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix(y,total_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33300fe",
   "metadata": {},
   "source": [
    "And the feature importance is"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b2e97c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:52.787752Z",
     "start_time": "2024-03-04T05:47:52.663033Z"
    }
   },
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, x.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152199a6",
   "metadata": {},
   "source": [
    "After a few tests, it seems that our simple model using only these engineered variables gave us approximately 60% accuracy. It is also evident that the number of links on a page plays a big role in detecting disaster tweets. Maybe this could be a variable used in other kernels to improve scores. Before ending the notebook, let's check the distribution of number of links for both disaster and non-disaster tweets."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3e2cc1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:47:56.265916Z",
     "start_time": "2024-03-04T05:47:56.166656Z"
    }
   },
   "source": [
    "training.loc[training[\"target\"] == 0.0, \"number_of_links\"].hist(alpha = 0.5);\n",
    "training.loc[training[\"target\"] == 1.0, \"number_of_links\"].hist(alpha = 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e5048",
   "metadata": {},
   "source": [
    "Indeed, most non-disaster tweets have no links at all, while most disaster ones have at least one.\n",
    "\n",
    "Now we'll just score the test dataset and make the submission."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bd5e8a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:48:00.183374Z",
     "start_time": "2024-03-04T05:48:00.175139Z"
    }
   },
   "source": [
    "preds = clf.predict(testing.loc[:, [\"number_of_tags\", \"number_of_links\", \"freqAdverbs\", \"freqVerbs\", \"freqAdjectives\", \"freqNouns\"]])\n",
    "testing[\"prediction\"] = preds"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e225e788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-04T05:58:17.001824Z",
     "start_time": "2024-03-04T05:58:16.994135Z"
    }
   },
   "source": [
    "submission = pd.read_csv(\"sample_submission_postag.csv\")\n",
    "submission[\"target\"] = preds.astype(int)\n",
    "#print(submission[\"target\"])\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ghaluh",
   "language": "python",
   "name": "ghaluh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
