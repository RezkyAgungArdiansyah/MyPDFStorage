{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b6df6b",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f466d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "headlines = pd.read_csv('abcnews-date-text.csv',\n",
    "                        parse_dates=[0], infer_datetime_format=True)\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['NumWords'] = headlines['headline_text'].apply(lambda x: len(x.split()))\n",
    "headlines[['NumWords']].hist(figsize=(12, 6), bins=10, xlabelsize=8, ylabelsize=8);\n",
    "plt.title(\"Distributon of number of words in the headlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['year'] = pd.DatetimeIndex(headlines['publish_date']).year\n",
    "headlines[['year']].hist(figsize=(12, 6), bins=10, xlabelsize=8, ylabelsize=8);\n",
    "plt.title(\"Distributon of number of headlines across years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['month'] = pd.DatetimeIndex(headlines['publish_date']).month\n",
    "headlines[['month']].hist(figsize=(12, 6), bins=10, xlabelsize=8, ylabelsize=8);\n",
    "plt.title(\"Distributon of number of headlines across months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['day'] = pd.DatetimeIndex(headlines['publish_date']).day\n",
    "headlines[['day']].hist(figsize=(12, 6), bins=10, xlabelsize=8, ylabelsize=8);\n",
    "plt.title(\"Distributon of number of headlines across days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['publish_date'] = pd.to_datetime(headlines['publish_date'])\n",
    "headlines = pd.DataFrame(headlines).set_index('publish_date') \n",
    "\n",
    "monthly_counts = headlines.resample('M').count()\n",
    "yearly_counts = headlines.resample('A').count()\n",
    "daily_counts = headlines.resample('D').count()\n",
    "fig, ax = plt.subplots(3, figsize=(18,16))\n",
    "ax[0].plot(daily_counts);\n",
    "ax[0].set_title('Daily Counts');\n",
    "ax[1].plot(monthly_counts);\n",
    "ax[1].set_title('Monthly Counts');\n",
    "ax[2].plot(yearly_counts);\n",
    "ax[2].set_title('Yearly Counts');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "all_words = ''.join([word for word in headlines['headline_text'][0:100000]])\n",
    "all_words\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.title(\"Some frequent words used in the headlines\", weight='bold', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb39bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "NON_ALPHANUM = re.compile(r'[\\W]')\n",
    "NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
    "def normalize_texts(texts):\n",
    "  normalized_texts = ''\n",
    "  lower = texts.lower()\n",
    "  no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n",
    "  no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n",
    "  return no_non_ascii\n",
    "  \n",
    "headlines['headline_text'] = headlines['headline_text'].apply(normalize_texts)\n",
    "headlines.head()\n",
    "headlines['headline_text'] = headlines['headline_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=10):\n",
    "  vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "  bag_of_words = vec.transform(corpus)\n",
    "  sum_words = bag_of_words.sum(axis=0) \n",
    "  words_freq = [(word, sum_words[0, idx]) for word, idx in   vec.vocabulary_.items()]\n",
    "  words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "  return words_freq[:n]\n",
    "words = []\n",
    "word_values = []\n",
    "for i,j in get_top_n_words(headlines['headline_text'],15):\n",
    "  words.append(i)\n",
    "  word_values.append(j)\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(range(len(words)), word_values);\n",
    "ax.set_xticks(range(len(words)));\n",
    "ax.set_xticklabels(words, rotation='vertical');\n",
    "ax.set_title('Top 15 words in the headlines dataset');\n",
    "ax.set_xlabel('Word');\n",
    "ax.set_ylabel('Number of occurences');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e1a82",
   "metadata": {},
   "source": [
    "# Method 1: Clustering using ‘wordtovec’ embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798faffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade gensim\n",
    "#importing wordtovec embeddings \n",
    "from gensim.models import KeyedVectors\n",
    "pretrained_embeddings_path = \"./GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec = KeyedVectors.load_word2vec_format(pretrained_embeddings_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3052dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'iraq'\n",
    "print('Word: {}'.format(word))\n",
    "print('First 20 values of embedding:\\n{}'.format(word2vec[word][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2148944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec.most_similar(positive=['woman', 'king'], negative=['man'], topn=3))\n",
    "print(word2vec.most_similar(positive=['Tennis', 'Ronaldo'], negative=['Soccer'], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = headlines.sample(frac = 0.02, random_state= 423)\n",
    "\n",
    "class WordVecVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = 300\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in texts.split() if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for texts in X\n",
    "        ])\n",
    "#representing each headline by the mean of word embeddings for the words used in the headlines.\n",
    "wtv_vect = WordVecVectorizer(word2vec)\n",
    "X_train_wtv = wtv_vect.transform(news['headline_text'])\n",
    "print(X_train_wtv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(\n",
    "    n_clusters=8, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=1e-04, random_state=0\n",
    ")\n",
    "y_km = km.fit_predict(X_train_wtv)\n",
    "df = pd.DataFrame({'headlines' :news['headline_text'], 'topic_cluster' :y_km })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd290fbc",
   "metadata": {},
   "source": [
    "# Method 2: Clustering using LDA ( Latent Dirichlet Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24fde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = headlines.sample(frac = 0.02, random_state= 423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122811dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)\n",
    "news_matrix = tf_vectorizer.fit_transform(news['headline_text'])\n",
    "#importing LDA\n",
    "from gensim import corpora, models\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#Fitting LDA\n",
    "lda = LatentDirichletAllocation(n_components=8, learning_method='online', \n",
    "                                          random_state=0, verbose=0, n_jobs = -1)\n",
    "lda_model = lda.fit(news_matrix)\n",
    "lda_matrix = lda_model.transform(news_matrix)\n",
    "lda_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ad9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = tf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "      \n",
    "        print(\"\\nTopic #%d:\" % topic_idx )\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda_model, news_matrix, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n",
    "                        n_iter=1000, verbose=1, random_state=0, angle=0.75)\n",
    "tsne_features = model.fit_transform(lda_matrix)\n",
    "df = pd.DataFrame(tsne_features)\n",
    "df['topic'] = lda_matrix.argmax(axis=1)\n",
    "df.columns = ['TSNE1', 'TSNE2', 'topic']\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.title('T-SNE plot of different headlines ( headlines are clustered among their topics)')\n",
    "ax = sns.scatterplot(x = 'TSNE1', y = 'TSNE2', hue = 'topic', data = df, legend = 'full')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ghaluh",
   "language": "python",
   "name": "ghaluh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
