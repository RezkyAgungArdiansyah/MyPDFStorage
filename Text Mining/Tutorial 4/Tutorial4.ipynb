{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7720d5e2-e7f3-4c16-b485-6ab6d6f7653b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>away</th>\n",
       "      <th>blue</th>\n",
       "      <th>compare</th>\n",
       "      <th>day</th>\n",
       "      <th>docs</th>\n",
       "      <th>doctor</th>\n",
       "      <th>keeps</th>\n",
       "      <th>learn</th>\n",
       "      <th>like</th>\n",
       "      <th>orange</th>\n",
       "      <th>prefer</th>\n",
       "      <th>scikit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   apple  away  blue  compare  day  docs  doctor  keeps  learn  like  orange  \\\n",
       "0      1     0     0        0    0     0       0      0      0     1       0   \n",
       "1      1     1     0        0    1     0       1      1      0     0       0   \n",
       "2      1     0     0        1    0     0       0      0      0     0       1   \n",
       "3      0     0     0        0    0     0       0      0      1     0       1   \n",
       "4      0     0     1        0    0     1       0      0      1     0       1   \n",
       "\n",
       "   prefer  scikit  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "2       0       0  \n",
       "3       1       1  \n",
       "4       0       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Define the documents\n",
    "corpus = [\"I'd like an apple\",\n",
    "          \"An apple a day keeps the doctor away\",\n",
    "          \"Never compare an apple to an orange\",\n",
    "          \"I prefer scikit-learn to Orange\",\n",
    "          \"The scikit-learn docs are Orange and Blue\"]\n",
    "\n",
    "# Create a document term matrix\n",
    "count_vectorizer = CountVectorizer(stop_words ='english')\n",
    "sparse_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, columns=count_vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e76875f0-4edf-4a2f-8c08-4771a11a4399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.31622777 0.40824829 0.         0.        ]\n",
      " [0.31622777 1.         0.25819889 0.         0.        ]\n",
      " [0.40824829 0.25819889 1.         0.28867513 0.25819889]\n",
      " [0.         0.         0.28867513 1.         0.67082039]\n",
      " [0.         0.         0.25819889 0.67082039 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d082f4f9-018e-4016-8304-c34b34aea812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.17668795, 0.27056873, 0.        , 0.        ],\n",
       "       [0.17668795, 1.        , 0.15439436, 0.        , 0.        ],\n",
       "       [0.27056873, 0.15439436, 1.        , 0.19635649, 0.16815247],\n",
       "       [0.        , 0.        , 0.19635649, 1.        , 0.54499756],\n",
       "       [0.        , 0.        , 0.16815247, 0.54499756, 1.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vect = TfidfVectorizer( stop_words='english')\n",
    "tfidf = vect.fit_transform(corpus)\n",
    "pairwise_similarity = tfidf * tfidf.T\n",
    "pairwise_similarity.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03f09fb6-53d9-41a2-9074-803a31efc5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.16815247 0.54499756        nan]\n"
     ]
    }
   ],
   "source": [
    "arr = pairwise_similarity.toarray()\n",
    "np.fill_diagonal(arr, np.nan)\n",
    "\n",
    "input_doc = \"The scikit-learn docs are Orange and Blue\"\n",
    "input_idx = corpus.index(input_doc)\n",
    "\n",
    "# [0.         0.         0.16815247 0.54499756        nan] arr[4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95508f55-4c66-48d7-a1b0-089a83b1e9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I prefer scikit-learn to Orange'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_idx =  np.nanargmax(arr[input_idx]) \n",
    "# result_idx = 3\n",
    "corpus[result_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cca9cae-e79d-404d-86ae-dc82d6c9c17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1) - Euclidien: 1.2832 - Manhattan: 2.9663\n",
      "(0, 2) - Euclidien: 1.2078 - Manhattan: 2.1134\n",
      "(0, 3) - Euclidien: 1.4142 - Manhattan: 3.3671\n",
      "(0, 4) - Euclidien: 1.4142 - Manhattan: 3.5991\n",
      "(1, 2) - Euclidien: 1.3005 - Manhattan: 3.2775\n",
      "(1, 3) - Euclidien: 1.4142 - Manhattan: 4.1938\n",
      "(1, 4) - Euclidien: 1.4142 - Manhattan: 4.4258\n",
      "(2, 3) - Euclidien: 1.2678 - Manhattan: 2.8707\n",
      "(2, 4) - Euclidien: 1.2898 - Manhattan: 3.2187\n",
      "(3, 4) - Euclidien: 0.9539 - Manhattan: 1.8335\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cityblock\n",
    "for idx_1, idx_2 in itertools.combinations(range(tfidf.shape[0]),2):\n",
    "    v1, v2 = map(lambda idx: tfidf.toarray()[idx],(idx_1,idx_2))\n",
    "    print(f\"{(idx_1,idx_2)}\"\\\n",
    "            f\" - Euclidien: {np.linalg.norm(v1-v2):.4f}\"\\\n",
    "            f\" - Manhattan: {cityblock(v1,v2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cedce235-0d0b-4103-ba62-fbc12d5f8d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1) - Pearson: -0.08180, 0.79051\n",
      "(0, 2) - Pearson: 0.10969, 0.72130\n",
      "(0, 3) - Pearson: -0.27388, 0.36521\n",
      "(0, 4) - Pearson: -0.32381, 0.28046\n",
      "(1, 2) - Pearson: -0.19374, 0.52594\n",
      "(1, 3) - Pearson: -0.51116, 0.07422\n",
      "(1, 4) - Pearson: -0.60433, 0.02869\n",
      "(2, 3) - Pearson: -0.08453, 0.78367\n",
      "(2, 4) - Pearson: -0.17345, 0.57092\n",
      "(3, 4) - Pearson: 0.31538, 0.29388\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "formater = lambda t:', '.join(('%.5f' % f) for f in t)\n",
    "for idx_1, idx_2 in itertools.combinations(range(tfidf.shape[0]),2):\n",
    "    v1, v2 = map(lambda idx: tfidf.toarray()[idx], (idx_1,idx_2))\n",
    "    print(f\"{(idx_1, idx_2)}\"\\\n",
    "         f\" - Pearson: {formater(stats.pearsonr(v1, v2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e9bbfab-cb77-4420-8358-4a455c97445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1) - Spearman: -0.05579, 0.85636\n",
      "(0, 2) - Spearman: 0.19543, 0.52226\n",
      "(0, 3) - Spearman: -0.27798, 0.35778\n",
      "(0, 4) - Spearman: -0.32487, 0.27879\n",
      "(1, 2) - Spearman: -0.18814, 0.53821\n",
      "(1, 3) - Spearman: -0.50753, 0.07665\n",
      "(1, 4) - Spearman: -0.59313, 0.03263\n",
      "(2, 3) - Spearman: -0.08206, 0.78985\n",
      "(2, 4) - Spearman: -0.18541, 0.54422\n",
      "(3, 4) - Spearman: 0.27317, 0.36651\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "formater = lambda t:', '.join(('%.5f' % f) for f in t)\n",
    "for idx_1, idx_2 in itertools.combinations(range(tfidf.shape[0]),2):\n",
    "    v1, v2 = map(lambda idx: tfidf.toarray()[idx], (idx_1,idx_2))\n",
    "    print(f\"{(idx_1, idx_2)}\"\\\n",
    "         f\" - Spearman: {formater(stats.spearmanr(v1, v2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e688f05a-7533-4803-a0bb-277d8820e855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>4018</td>\n",
       "      <td>labour plans maternity pay rise maternity pay ...</td>\n",
       "      <td>no seasonal lift for house market a swathe of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>4019</td>\n",
       "      <td>high fuel costs hit us airlines two of the lar...</td>\n",
       "      <td>new media battle for bafta awards the bbc lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>4020</td>\n",
       "      <td>britons growing  digitally obese  gadget lover...</td>\n",
       "      <td>film star fox behind theatre bid leading actor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>4021</td>\n",
       "      <td>holmes is hit by hamstring injury kelly holmes...</td>\n",
       "      <td>tsunami  to hit sri lanka banks  sri lanka s b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>4022</td>\n",
       "      <td>nuclear dumpsite  plan attacked plans to allow...</td>\n",
       "      <td>x factor show gets second series tv talent sho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4023 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unique_ID                                              text1  \\\n",
       "0             0  savvy searchers fail to spot ads internet sear...   \n",
       "1             1  millions to miss out on the net by 2025  40% o...   \n",
       "2             2  young debut cut short by ginepri fifteen-year-...   \n",
       "3             3  diageo to buy us wine firm diageo  the world s...   \n",
       "4             4  be careful how you code a new european directi...   \n",
       "...         ...                                                ...   \n",
       "4018       4018  labour plans maternity pay rise maternity pay ...   \n",
       "4019       4019  high fuel costs hit us airlines two of the lar...   \n",
       "4020       4020  britons growing  digitally obese  gadget lover...   \n",
       "4021       4021  holmes is hit by hamstring injury kelly holmes...   \n",
       "4022       4022  nuclear dumpsite  plan attacked plans to allow...   \n",
       "\n",
       "                                                  text2  \n",
       "0     newcastle 2-1 bolton kieron dyer smashed home ...  \n",
       "1     nasdaq planning $100m share sale the owner of ...  \n",
       "2     ruddock backs yapp s credentials wales coach m...  \n",
       "3     mci shares climb on takeover bid shares in us ...  \n",
       "4     media gadgets get moving pocket-sized devices ...  \n",
       "...                                                 ...  \n",
       "4018  no seasonal lift for house market a swathe of ...  \n",
       "4019  new media battle for bafta awards the bbc lead...  \n",
       "4020  film star fox behind theatre bid leading actor...  \n",
       "4021  tsunami  to hit sri lanka banks  sri lanka s b...  \n",
       "4022  x factor show gets second series tv talent sho...  \n",
       "\n",
       "[4023 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Text_Similarity_Dataset.csv')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "340f9ab9-514b-4ad9-a4c4-538eef6a376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from pandas import DataFrame, merge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cart_df = pd.read_csv('Text_Similarity_Dataset.csv')\n",
    "\n",
    "# cart_product = list(itertools.product(df['text1'],df['text2']))\n",
    "\n",
    "# cart_df = pd.DataFrame(cart_product, columns=['text1','text2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c999751-0de7-4199-acb7-cb4a40794ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cart_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dbfee1-19c1-4791-973d-e56c3b2007a4",
   "metadata": {},
   "source": [
    "# Count_Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad233c4-9dab-45e9-b38b-939e774f6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity = []\n",
    "for i in range(len(df)):\n",
    "    doc1=df['text1'][i]\n",
    "    doc2=df['text2'][i]\n",
    "    docs=(doc1,doc2)\n",
    "    matrix = CountVectorizer().fit_transform(docs)\n",
    "    cosine_sim = cosine_similarity(matrix[0], matrix[1])\n",
    "    cos_similarity.append(cosine_sim)\n",
    "df['Count_Vectorizer'] = cos_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c86b27d-11ea-49fa-ab73-fd684b82b6eb",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae1ae43-50c6-46ba-b3ed-eb0cc2717f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "sim_list = []\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "for i in range(len(df)):\n",
    "    doc1=df['text1'][i]\n",
    "    doc2=df['text2'][i]\n",
    "    docs=(doc1,doc2)\n",
    "    tfidf_matrix = vect.fit_transform(docs)\n",
    "    sim = cosine_similarity(tfidf_matrix[0],tfidf_matrix[1])\n",
    "    sim_list.append(sim)\n",
    "df['TF_IDF_Similarity'] = sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95595de6-7ed4-4813-b482-3eefa35e3947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>Count_Vectorizer</th>\n",
       "      <th>TF_IDF_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "      <td>[[0.5121263855362397]]</td>\n",
       "      <td>[[0.018400340744745478]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "      <td>[[0.6867717040468696]]</td>\n",
       "      <td>[[0.033814752892231334]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "      <td>[[0.6365378822840906]]</td>\n",
       "      <td>[[0.05273885434484543]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "      <td>[[0.4020487524843295]]</td>\n",
       "      <td>[[0.04086775571951435]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "      <td>[[0.7648826352194371]]</td>\n",
       "      <td>[[0.05168140016827307]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>4018</td>\n",
       "      <td>labour plans maternity pay rise maternity pay ...</td>\n",
       "      <td>no seasonal lift for house market a swathe of ...</td>\n",
       "      <td>[[0.6487492098922014]]</td>\n",
       "      <td>[[0.07389229110279227]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>4019</td>\n",
       "      <td>high fuel costs hit us airlines two of the lar...</td>\n",
       "      <td>new media battle for bafta awards the bbc lead...</td>\n",
       "      <td>[[0.6605570566273307]]</td>\n",
       "      <td>[[0.05014187171483796]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>4020</td>\n",
       "      <td>britons growing  digitally obese  gadget lover...</td>\n",
       "      <td>film star fox behind theatre bid leading actor...</td>\n",
       "      <td>[[0.6799340870442956]]</td>\n",
       "      <td>[[0.0719717480291427]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>4021</td>\n",
       "      <td>holmes is hit by hamstring injury kelly holmes...</td>\n",
       "      <td>tsunami  to hit sri lanka banks  sri lanka s b...</td>\n",
       "      <td>[[0.5454415845071072]]</td>\n",
       "      <td>[[0.02894578397832087]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>4022</td>\n",
       "      <td>nuclear dumpsite  plan attacked plans to allow...</td>\n",
       "      <td>x factor show gets second series tv talent sho...</td>\n",
       "      <td>[[0.57475869010649]]</td>\n",
       "      <td>[[0.028675162887945638]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4023 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unique_ID                                              text1  \\\n",
       "0             0  savvy searchers fail to spot ads internet sear...   \n",
       "1             1  millions to miss out on the net by 2025  40% o...   \n",
       "2             2  young debut cut short by ginepri fifteen-year-...   \n",
       "3             3  diageo to buy us wine firm diageo  the world s...   \n",
       "4             4  be careful how you code a new european directi...   \n",
       "...         ...                                                ...   \n",
       "4018       4018  labour plans maternity pay rise maternity pay ...   \n",
       "4019       4019  high fuel costs hit us airlines two of the lar...   \n",
       "4020       4020  britons growing  digitally obese  gadget lover...   \n",
       "4021       4021  holmes is hit by hamstring injury kelly holmes...   \n",
       "4022       4022  nuclear dumpsite  plan attacked plans to allow...   \n",
       "\n",
       "                                                  text2  \\\n",
       "0     newcastle 2-1 bolton kieron dyer smashed home ...   \n",
       "1     nasdaq planning $100m share sale the owner of ...   \n",
       "2     ruddock backs yapp s credentials wales coach m...   \n",
       "3     mci shares climb on takeover bid shares in us ...   \n",
       "4     media gadgets get moving pocket-sized devices ...   \n",
       "...                                                 ...   \n",
       "4018  no seasonal lift for house market a swathe of ...   \n",
       "4019  new media battle for bafta awards the bbc lead...   \n",
       "4020  film star fox behind theatre bid leading actor...   \n",
       "4021  tsunami  to hit sri lanka banks  sri lanka s b...   \n",
       "4022  x factor show gets second series tv talent sho...   \n",
       "\n",
       "            Count_Vectorizer         TF_IDF_Similarity  \n",
       "0     [[0.5121263855362397]]  [[0.018400340744745478]]  \n",
       "1     [[0.6867717040468696]]  [[0.033814752892231334]]  \n",
       "2     [[0.6365378822840906]]   [[0.05273885434484543]]  \n",
       "3     [[0.4020487524843295]]   [[0.04086775571951435]]  \n",
       "4     [[0.7648826352194371]]   [[0.05168140016827307]]  \n",
       "...                      ...                       ...  \n",
       "4018  [[0.6487492098922014]]   [[0.07389229110279227]]  \n",
       "4019  [[0.6605570566273307]]   [[0.05014187171483796]]  \n",
       "4020  [[0.6799340870442956]]    [[0.0719717480291427]]  \n",
       "4021  [[0.5454415845071072]]   [[0.02894578397832087]]  \n",
       "4022    [[0.57475869010649]]  [[0.028675162887945638]]  \n",
       "\n",
       "[4023 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "609f6f83-7731-4bef-bf75-79672b49aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distance = cart_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e30df97f-f869-4ad4-a20e-3e65c41ead34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>euclid_distance</th>\n",
       "      <th>manhattan_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "      <td>1.401142</td>\n",
       "      <td>20.889044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "      <td>1.390097</td>\n",
       "      <td>18.716633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "      <td>1.376416</td>\n",
       "      <td>19.894704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "      <td>1.385014</td>\n",
       "      <td>14.744520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "      <td>1.377185</td>\n",
       "      <td>24.002366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>4018</td>\n",
       "      <td>labour plans maternity pay rise maternity pay ...</td>\n",
       "      <td>no seasonal lift for house market a swathe of ...</td>\n",
       "      <td>1.360961</td>\n",
       "      <td>17.934720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>4019</td>\n",
       "      <td>high fuel costs hit us airlines two of the lar...</td>\n",
       "      <td>new media battle for bafta awards the bbc lead...</td>\n",
       "      <td>1.378302</td>\n",
       "      <td>16.993757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>4020</td>\n",
       "      <td>britons growing  digitally obese  gadget lover...</td>\n",
       "      <td>film star fox behind theatre bid leading actor...</td>\n",
       "      <td>1.362372</td>\n",
       "      <td>21.289230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>4021</td>\n",
       "      <td>holmes is hit by hamstring injury kelly holmes...</td>\n",
       "      <td>tsunami  to hit sri lanka banks  sri lanka s b...</td>\n",
       "      <td>1.393596</td>\n",
       "      <td>16.081390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>4022</td>\n",
       "      <td>nuclear dumpsite  plan attacked plans to allow...</td>\n",
       "      <td>x factor show gets second series tv talent sho...</td>\n",
       "      <td>1.393790</td>\n",
       "      <td>15.868088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4023 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unique_ID                                              text1  \\\n",
       "0             0  savvy searchers fail to spot ads internet sear...   \n",
       "1             1  millions to miss out on the net by 2025  40% o...   \n",
       "2             2  young debut cut short by ginepri fifteen-year-...   \n",
       "3             3  diageo to buy us wine firm diageo  the world s...   \n",
       "4             4  be careful how you code a new european directi...   \n",
       "...         ...                                                ...   \n",
       "4018       4018  labour plans maternity pay rise maternity pay ...   \n",
       "4019       4019  high fuel costs hit us airlines two of the lar...   \n",
       "4020       4020  britons growing  digitally obese  gadget lover...   \n",
       "4021       4021  holmes is hit by hamstring injury kelly holmes...   \n",
       "4022       4022  nuclear dumpsite  plan attacked plans to allow...   \n",
       "\n",
       "                                                  text2  euclid_distance  \\\n",
       "0     newcastle 2-1 bolton kieron dyer smashed home ...         1.401142   \n",
       "1     nasdaq planning $100m share sale the owner of ...         1.390097   \n",
       "2     ruddock backs yapp s credentials wales coach m...         1.376416   \n",
       "3     mci shares climb on takeover bid shares in us ...         1.385014   \n",
       "4     media gadgets get moving pocket-sized devices ...         1.377185   \n",
       "...                                                 ...              ...   \n",
       "4018  no seasonal lift for house market a swathe of ...         1.360961   \n",
       "4019  new media battle for bafta awards the bbc lead...         1.378302   \n",
       "4020  film star fox behind theatre bid leading actor...         1.362372   \n",
       "4021  tsunami  to hit sri lanka banks  sri lanka s b...         1.393596   \n",
       "4022  x factor show gets second series tv talent sho...         1.393790   \n",
       "\n",
       "      manhattan_distance  \n",
       "0              20.889044  \n",
       "1              18.716633  \n",
       "2              19.894704  \n",
       "3              14.744520  \n",
       "4              24.002366  \n",
       "...                  ...  \n",
       "4018           17.934720  \n",
       "4019           16.993757  \n",
       "4020           21.289230  \n",
       "4021           16.081390  \n",
       "4022           15.868088  \n",
       "\n",
       "[4023 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cityblock\n",
    "import numpy as np\n",
    "Vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "euclidean_distance = []\n",
    "manhattan_distance = []\n",
    "for i in range(len(df)):\n",
    "    doc1=df['text1'][i]\n",
    "    doc2=df['text2'][i]\n",
    "    docs=(doc1,doc2)\n",
    "    tfidf_matrix = Vectorizer.fit_transform(docs)\n",
    "    euclid_distance = np.linalg.norm(tfidf_matrix.toarray()[0]-tfidf_matrix.toarray()[1])\n",
    "    manh_distance = cityblock(tfidf_matrix.toarray()[0],tfidf_matrix.toarray()[1])\n",
    "    euclidean_distance.append(euclid_distance)\n",
    "    manhattan_distance.append(manh_distance)\n",
    "df_distance['euclid_distance'] = euclidean_distance\n",
    "df_distance['manhattan_distance'] = manhattan_distance\n",
    "df_distance\n",
    "    # euclid_distance = np.linalg.norm(tfidf_matrix[0]-tfidf_matrix[1]) \n",
    "    # Manhattan_distance = cityblock(tfidf_matrix[0],tfidf_matrix[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33d90e6f-394b-483f-b61e-c491a1cdf18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distance.to_csv('df_distance.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d64d3c47-3950-474b-bc7e-f7bdeff4e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_cosines_similarity.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
