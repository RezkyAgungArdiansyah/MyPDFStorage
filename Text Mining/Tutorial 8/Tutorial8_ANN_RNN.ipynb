{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580ceada",
   "metadata": {},
   "source": [
    "# 1. Simple ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0309f0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.4476407170295715\n",
      "Epoch 50, Loss: 0.38067219592630863\n",
      "Epoch 100, Loss: 0.2197433365508914\n",
      "Epoch 150, Loss: 0.21372364863054827\n",
      "Epoch 200, Loss: 0.3830622520763427\n",
      "Epoch 250, Loss: 0.20904743735445663\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1)  # Sets the seed for PyTorch random number generators. This is important to ensure that the behavior of any operation that introduces randomness, such as the initialization of weights in neural networks and the shuffling of data in DataLoaders, is consistent across runs. Using a fixed seed value like 42 ensures that every execution of this code will produce the same results given the same inputs and model configuration. This is essential for debugging and comparing the performance impacts of any changes in a controlled manner.\n",
    "\n",
    "# Text preprocessing: Convert the entire string to lowercase and split it into individual words, removing commas.\n",
    "# This normalization helps ensure the model treats variations of the same word (e.g., with and without punctuation) as the same word.\n",
    "text = \"Tell me and I forget, teach me and I may remember, involve me and I learn\"\n",
    "words = text.lower().replace(',', '').split()\n",
    "\n",
    "# Vocabulary creation: Map each unique word to a unique index. This numerical representation of words is necessary\n",
    "# because models can only process numbers, not text.\n",
    "vocab = {word: i for i, word in enumerate(set(words))}\n",
    "vocab_size = len(vocab)  # The size of the vocabulary\n",
    "\n",
    "# Prepare the input and target pairs for training the model.\n",
    "# The input will be sequences of words (context), and the target will be the word that follows the context.\n",
    "inputs = []\n",
    "targets = []\n",
    "context_size = 3  # The number of words considered as context for predicting the next word.\n",
    "\n",
    "# Generate sequences of contexts and their corresponding target words.\n",
    "for i in range(len(words) - context_size):\n",
    "    input_idx = [vocab[words[j]] for j in range(i, i + context_size)]  # Indices of the context words\n",
    "    target_idx = vocab[words[i + context_size]]  # Index of the target word\n",
    "    inputs.append(input_idx)\n",
    "    targets.append(target_idx)\n",
    "\n",
    "# Convert the lists of indices into PyTorch tensors, which are optimized for performance in training.\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Packaging the tensors into a dataset and a DataLoader for efficient batching during training.\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Define the neural network model structure.\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  # Converts word indices to embeddings.\n",
    "        self.fc1 = nn.Linear(embedding_dim * context_size, 50)     # First fully connected layer.\n",
    "        self.fc2 = nn.Linear(50, 20)                               # Second fully connected layer.\n",
    "        self.fc3 = nn.Linear(20, vocab_size)                       # Output layer that predicts the next word.\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Define the forward pass through the network.\n",
    "        embeds = self.embeddings(inputs).view((inputs.shape[0], -1))  # Flatten the embeddings.\n",
    "        out = torch.relu(self.fc1(embeds))                           # Apply ReLU activation function for non-linearity.\n",
    "        out = torch.relu(self.fc2(out))                              # Another ReLU activation for hidden layer.\n",
    "        out = self.fc3(out)                                          # Output layer; no activation (logits).\n",
    "        return out\n",
    "\n",
    "# Parameters for the model.\n",
    "embedding_dim = 10  # Size of the embedding vectors. Smaller for simplicity in this example.\n",
    "\n",
    "# Initialize the model, loss function, and optimizer.\n",
    "model = SimpleNN(vocab_size, embedding_dim)\n",
    "loss_function = nn.CrossEntropyLoss()  # Suitable for classification tasks with multiple classes.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001.\n",
    "\n",
    "# Training loop: Iterate over the data multiple times (epochs) to optimize the model parameters.\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in dataloader:\n",
    "        model.zero_grad()            # Clear old gradients from the last step.\n",
    "        log_probs = model(context)   # Calculate the log probabilities of the next word.\n",
    "        loss = loss_function(log_probs, target)  # Compute the cross-entropy loss.\n",
    "        loss.backward()              # Perform backpropagation to calculate gradients.\n",
    "        optimizer.step()             # Update the weights.\n",
    "        total_loss += loss.item()    # Accumulate the loss for monitoring.\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / len(dataloader)}')  # Print loss every 50 epochs for monitoring.\n",
    "\n",
    "# Function to predict the next word given a string of text\n",
    "def predict(text):\n",
    "    \"\"\"\n",
    "    Predicts the next word based on the last few words of the input text using the trained model.\n",
    "    This function preprocesses the text to fit the model's training setup, then performs a forward pass to predict.\n",
    "    \"\"\"\n",
    "    words = text.lower().replace(',', '').split()\n",
    "    input_idx = [vocab.get(word, 0) for word in words[-context_size:]]  # Convert last few words to indices.\n",
    "    input_tensor = torch.tensor([input_idx], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        log_probs = model(input_tensor)\n",
    "    return max(zip(log_probs[0].exp(), vocab.keys()), key=lambda p: p[0])[1]  # Return the most probable word.\n",
    "\n",
    "# Test the prediction function\n",
    "print(predict(\"teach me and\"))  # Expected to output 'I', given the training context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5918a",
   "metadata": {},
   "source": [
    "## 1.1. Testing the prediction function with various input scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7170332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remember\n"
     ]
    }
   ],
   "source": [
    "# Test case with a context seen during training\n",
    "# \"and I may\" is a context similar to the training data, so the model should predict accurately\n",
    "print(predict(\"and I may\"))  # Expected output: 'remember', as it follows this context in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0963c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remember\n"
     ]
    }
   ],
   "source": [
    "# Test case with a partially unseen context\n",
    "# \"and I do\" includes words seen during training but not this exact sequence\n",
    "# The model might struggle or default to a more frequent or statistically likely word\n",
    "print(predict(\"and I do\"))   # Output might be less accurate due to unseen sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4c8750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n"
     ]
    }
   ],
   "source": [
    "# Test case with entirely unseen context\n",
    "# \"tell me next\" includes 'next' which never appears in the training data\n",
    "# This showcases how the model handles completely novel input\n",
    "print(predict(\"tell me next\"))  # Output is unpredictable, illustrating limitations with unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a53ec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "involve\n"
     ]
    }
   ],
   "source": [
    "# Test case with another unseen context\n",
    "# \"do not tell\" is not only a completely unseen phrase but also contains \"not,\" which is not in the training data\n",
    "# This tests the model's handling of completely novel words and sequences\n",
    "print(predict(\"do not tell\"))   # The prediction is likely to be unreliable due to the novel context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98108a0",
   "metadata": {},
   "source": [
    "## 1.2. Enhanced prediction function with detailed probability outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adcce32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: 'teach me and'\n",
      "Word: \"i\", Score: 0.9997\n",
      "Word: \"teach\", Score: 0.0001\n",
      "Word: \"remember\", Score: 0.0000\n",
      "Word: \"me\", Score: 0.0000\n",
      "Word: \"involve\", Score: 0.0000\n",
      "Word: \"forget\", Score: 0.0000\n",
      "Word: \"may\", Score: 0.0000\n",
      "Word: \"learn\", Score: 0.0000\n",
      "Word: \"and\", Score: 0.0000\n",
      "Word: \"tell\", Score: 0.0000\n",
      "\n",
      "\n",
      "Context: 'and I may'\n",
      "Word: \"remember\", Score: 0.9979\n",
      "Word: \"teach\", Score: 0.0008\n",
      "Word: \"me\", Score: 0.0005\n",
      "Word: \"i\", Score: 0.0003\n",
      "Word: \"involve\", Score: 0.0003\n",
      "Word: \"and\", Score: 0.0001\n",
      "Word: \"forget\", Score: 0.0000\n",
      "Word: \"tell\", Score: 0.0000\n",
      "Word: \"learn\", Score: 0.0000\n",
      "Word: \"may\", Score: 0.0000\n",
      "\n",
      "\n",
      "Context: 'and I do'\n",
      "Word: \"remember\", Score: 0.9979\n",
      "Word: \"teach\", Score: 0.0008\n",
      "Word: \"me\", Score: 0.0005\n",
      "Word: \"i\", Score: 0.0003\n",
      "Word: \"involve\", Score: 0.0003\n",
      "Word: \"and\", Score: 0.0001\n",
      "Word: \"forget\", Score: 0.0000\n",
      "Word: \"tell\", Score: 0.0000\n",
      "Word: \"learn\", Score: 0.0000\n",
      "Word: \"may\", Score: 0.0000\n",
      "\n",
      "\n",
      "Context: 'tell me next'\n",
      "Word: \"i\", Score: 0.8529\n",
      "Word: \"remember\", Score: 0.1205\n",
      "Word: \"me\", Score: 0.0173\n",
      "Word: \"involve\", Score: 0.0043\n",
      "Word: \"teach\", Score: 0.0034\n",
      "Word: \"forget\", Score: 0.0007\n",
      "Word: \"and\", Score: 0.0003\n",
      "Word: \"learn\", Score: 0.0002\n",
      "Word: \"may\", Score: 0.0002\n",
      "Word: \"tell\", Score: 0.0001\n",
      "\n",
      "\n",
      "Context: 'do not tell'\n",
      "Word: \"involve\", Score: 0.7496\n",
      "Word: \"remember\", Score: 0.1195\n",
      "Word: \"me\", Score: 0.0737\n",
      "Word: \"i\", Score: 0.0134\n",
      "Word: \"and\", Score: 0.0122\n",
      "Word: \"forget\", Score: 0.0082\n",
      "Word: \"learn\", Score: 0.0072\n",
      "Word: \"tell\", Score: 0.0069\n",
      "Word: \"may\", Score: 0.0050\n",
      "Word: \"teach\", Score: 0.0043\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the predict function to return all words and their probabilities\n",
    "def predict_all_scores(text):\n",
    "    # Convert the input text to lowercase and split it into words, removing any commas\n",
    "    words = text.lower().replace(',', '').split()\n",
    "\n",
    "    # Convert words to indices. Use a default index (0) if a word is not found in the vocabulary.\n",
    "    # This handling is basic and assumes '0' index won't cause misinterpretations, typically it should be handled more carefully.\n",
    "    input_idx = [vocab.get(word, 0) for word in words[-context_size:]]\n",
    "\n",
    "    # Convert the list of indices into a tensor suitable for model input\n",
    "    input_tensor = torch.tensor([input_idx], dtype=torch.long)\n",
    "\n",
    "    # Disable gradient calculations, useful for inference to save memory and computations\n",
    "    with torch.no_grad():\n",
    "        # Pass the input tensor through the model to get the log probabilities of each class (word in this case)\n",
    "        log_probs = model(input_tensor)\n",
    "\n",
    "        # Apply softmax to the log probabilities to convert them to actual probabilities\n",
    "        # The softmax function converts raw model outputs into a probability distribution\n",
    "        probs = torch.softmax(log_probs, dim=1)\n",
    "\n",
    "    # Extract the probabilities for each word in the vocabulary\n",
    "    all_probs = probs.squeeze().tolist()  # Convert the tensor of probabilities to a list\n",
    "\n",
    "    # Create a dictionary mapping each word to its predicted probability\n",
    "    predicted_words_scores = {word: prob for word, idx in vocab.items() for prob in [all_probs[idx]]}\n",
    "\n",
    "    # Sort the dictionary by probabilities in descending order to see the most likely words first\n",
    "    predicted_words_scores = dict(sorted(predicted_words_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Return the sorted dictionary of words and their associated probabilities\n",
    "    return predicted_words_scores\n",
    "\n",
    "# Test the updated prediction function with different contexts\n",
    "# This will print out all words in the vocabulary with their associated probability scores\n",
    "\n",
    "contexts = [\"teach me and\", \"and I may\", \"and I do\", \"tell me next\", \"do not tell\"]\n",
    "for context in contexts:\n",
    "    predicted_scores = predict_all_scores(context)\n",
    "    print(f\"Context: '{context}'\")\n",
    "    for word, score in predicted_scores.items():\n",
    "        print(f'Word: \"{word}\", Score: {score:.4f}')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758d439",
   "metadata": {},
   "source": [
    "# 2. Simple One-Layer RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4d60cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3606629967689514\n",
      "Epoch 50, Loss: 0.936677560210228\n",
      "Epoch 100, Loss: 0.3950011841952801\n",
      "Epoch 150, Loss: 0.29567510448396206\n",
      "Epoch 200, Loss: 0.4735059132799506\n",
      "Epoch 250, Loss: 0.24506466882303357\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1)  # Sets the seed for PyTorch random number generators. This is important to ensure that the behavior of any operation that introduces randomness, such as the initialization of weights in neural networks and the shuffling of data in DataLoaders, is consistent across runs. Using a fixed seed value like 42 ensures that every execution of this code will produce the same results given the same inputs and model configuration. This is essential for debugging and comparing the performance impacts of any changes in a controlled manner.\n",
    "\n",
    "# Text preprocessing\n",
    "text = \"Tell me and I forget, teach me and I may remember, involve me and I learn\"\n",
    "words = text.lower().replace(',', '').split()\n",
    "vocab = {word: i for i, word in enumerate(set(words))}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Prepare data for the model\n",
    "inputs = []\n",
    "targets = []\n",
    "context_size = 3\n",
    "\n",
    "for i in range(len(words) - context_size):\n",
    "    input_idx = [vocab[words[j]] for j in range(i, i + context_size)]\n",
    "    target_idx = vocab[words[i + context_size]]\n",
    "    inputs.append(input_idx)\n",
    "    targets.append(target_idx)\n",
    "\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Define an RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # RNN layer, unidirectional and one layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Input shape: (batch_size, sequence_length)\n",
    "        embeds = self.embeddings(inputs)  # (batch_size, sequence_length, embedding_dim)\n",
    "        rnn_out, _ = self.rnn(embeds)     # (batch_size, sequence_length, hidden_dim)\n",
    "        # We use the last RNN output as the representation of the sequence\n",
    "        final_output = rnn_out[:, -1, :] # (batch_size, hidden_dim)\n",
    "        out = self.fc(final_output)       # (batch_size, vocab_size)\n",
    "        return out\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "\n",
    "# Initialize the RNN model, loss function, and optimizer\n",
    "model = SimpleRNN(vocab_size, embedding_dim, hidden_dim)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in dataloader:\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / len(dataloader)}')\n",
    "        \n",
    "# Prediction function\n",
    "def predict(text):\n",
    "    words = text.lower().replace(',', '').split()\n",
    "    input_idx = [vocab.get(word, 0) for word in words[-context_size:]]\n",
    "    input_tensor = torch.tensor([input_idx], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        log_probs = model(input_tensor)\n",
    "    return max(zip(log_probs[0].exp(), vocab.keys()), key=lambda p: p[0])[1]\n",
    "\n",
    "# Test the prediction function\n",
    "print(predict(\"teach me and\"))  # Expected to output 'I', given the training context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25991196",
   "metadata": {},
   "source": [
    "## 2.1. Testing the prediction function with various input scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c795bcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remember\n"
     ]
    }
   ],
   "source": [
    "# Test case with a context seen during training\n",
    "# \"and I may\" is a context similar to the training data, so the model should predict accurately\n",
    "print(predict(\"and I may\"))  # Expected output: 'remember', as it follows this context in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebaae090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remember\n"
     ]
    }
   ],
   "source": [
    "# Test case with a partially unseen context\n",
    "# \"and I do\" includes words seen during training but not this exact sequence\n",
    "# The model might struggle or default to a more frequent or statistically likely word\n",
    "print(predict(\"and I do\"))   # Output might be less accurate due to unseen sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac33d778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n"
     ]
    }
   ],
   "source": [
    "# Test case with entirely unseen context\n",
    "# \"tell me next\" includes 'next' which never appears in the training data\n",
    "# This showcases how the model handles completely novel input\n",
    "print(predict(\"tell me next\"))  # Output is unpredictable, illustrating limitations with unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64c2d331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "involve\n"
     ]
    }
   ],
   "source": [
    "# Test case with another unseen context\n",
    "# \"do not tell\" is not only a completely unseen phrase but also contains \"not,\" which is not in the training data\n",
    "# This tests the model's handling of completely novel words and sequences\n",
    "print(predict(\"do not tell\"))   # The prediction is likely to be unreliable due to the novel context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae963eb",
   "metadata": {},
   "source": [
    "## 2.2. Enhanced prediction function with detailed probability outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39bbfb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: 'teach me and'\n",
      "Word: \"i\", Score: 0.9861\n",
      "Word: \"learn\", Score: 0.0031\n",
      "Word: \"forget\", Score: 0.0027\n",
      "Word: \"may\", Score: 0.0023\n",
      "Word: \"me\", Score: 0.0017\n",
      "Word: \"tell\", Score: 0.0013\n",
      "Word: \"involve\", Score: 0.0012\n",
      "Word: \"and\", Score: 0.0011\n",
      "Word: \"remember\", Score: 0.0005\n",
      "Word: \"teach\", Score: 0.0000\n",
      "\n",
      "\n",
      "Context: 'and I may'\n",
      "Word: \"remember\", Score: 0.9604\n",
      "Word: \"teach\", Score: 0.0101\n",
      "Word: \"me\", Score: 0.0092\n",
      "Word: \"learn\", Score: 0.0071\n",
      "Word: \"and\", Score: 0.0057\n",
      "Word: \"tell\", Score: 0.0025\n",
      "Word: \"may\", Score: 0.0025\n",
      "Word: \"forget\", Score: 0.0016\n",
      "Word: \"i\", Score: 0.0007\n",
      "Word: \"involve\", Score: 0.0002\n",
      "\n",
      "\n",
      "Context: 'and I do'\n",
      "Word: \"remember\", Score: 0.9604\n",
      "Word: \"teach\", Score: 0.0101\n",
      "Word: \"me\", Score: 0.0092\n",
      "Word: \"learn\", Score: 0.0071\n",
      "Word: \"and\", Score: 0.0057\n",
      "Word: \"tell\", Score: 0.0025\n",
      "Word: \"may\", Score: 0.0025\n",
      "Word: \"forget\", Score: 0.0016\n",
      "Word: \"i\", Score: 0.0007\n",
      "Word: \"involve\", Score: 0.0002\n",
      "\n",
      "\n",
      "Context: 'tell me next'\n",
      "Word: \"i\", Score: 0.8843\n",
      "Word: \"and\", Score: 0.0451\n",
      "Word: \"remember\", Score: 0.0212\n",
      "Word: \"learn\", Score: 0.0128\n",
      "Word: \"tell\", Score: 0.0101\n",
      "Word: \"me\", Score: 0.0088\n",
      "Word: \"forget\", Score: 0.0079\n",
      "Word: \"may\", Score: 0.0068\n",
      "Word: \"involve\", Score: 0.0026\n",
      "Word: \"teach\", Score: 0.0005\n",
      "\n",
      "\n",
      "Context: 'do not tell'\n",
      "Word: \"involve\", Score: 0.6523\n",
      "Word: \"i\", Score: 0.0699\n",
      "Word: \"teach\", Score: 0.0559\n",
      "Word: \"forget\", Score: 0.0540\n",
      "Word: \"and\", Score: 0.0525\n",
      "Word: \"may\", Score: 0.0409\n",
      "Word: \"learn\", Score: 0.0330\n",
      "Word: \"tell\", Score: 0.0257\n",
      "Word: \"me\", Score: 0.0147\n",
      "Word: \"remember\", Score: 0.0012\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Improved Prediction Function\n",
    "def predict_all_scores(text):\n",
    "    \"\"\"\n",
    "    Predicts the probabilities of all possible next words based on the input text.\n",
    "    \n",
    "    Args:\n",
    "    text (str): A string of text from which the last few words are taken as context for prediction.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary of words and their associated probabilities, sorted by likelihood.\n",
    "    \"\"\"\n",
    "    # Normalize the input text: convert to lowercase and remove commas for consistent preprocessing\n",
    "    words = text.lower().replace(',', '').split()\n",
    "\n",
    "    # Convert the last few words to indices using the vocabulary.\n",
    "    # Use a default index (0) if a word is not found in the vocabulary.\n",
    "    input_idx = [vocab.get(word, 0) for word in words[-context_size:]]\n",
    "\n",
    "    # Create a tensor from the word indices, suitable for model input\n",
    "    input_tensor = torch.tensor([input_idx], dtype=torch.long)\n",
    "\n",
    "    # Disable gradient calculations for efficiency during inference\n",
    "    with torch.no_grad():\n",
    "        # Pass the tensor through the model to get log probabilities for each class (word)\n",
    "        log_probs = model(input_tensor)\n",
    "\n",
    "        # Apply softmax to convert log probabilities to actual probabilities\n",
    "        probs = torch.softmax(log_probs, dim=1)\n",
    "\n",
    "    # Convert the probabilities tensor to a list for easier processing\n",
    "    all_probs = probs.squeeze().tolist()\n",
    "\n",
    "    # Map each word in the vocabulary to its predicted probability\n",
    "    predicted_words_scores = {word: prob for word, idx in vocab.items() for prob in [all_probs[idx]]}\n",
    "\n",
    "    # Sort the predictions by their probabilities in descending order\n",
    "    predicted_words_scores = dict(sorted(predicted_words_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return predicted_words_scores\n",
    "\n",
    "# Test the updated prediction function with different contexts\n",
    "contexts = [\"teach me and\", \"and I may\", \"and I do\", \"tell me next\", \"do not tell\"]\n",
    "for context in contexts:\n",
    "    predicted_scores = predict_all_scores(context)\n",
    "    print(f\"Context: '{context}'\")\n",
    "    for word, score in predicted_scores.items():\n",
    "        print(f'Word: \"{word}\", Score: {score:.4f}')\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69dcfba",
   "metadata": {},
   "source": [
    "## 2.3. Two-Layer RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "517fcd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.298316478729248\n",
      "Epoch 50, Loss: 0.705595888197422\n",
      "Epoch 100, Loss: 0.3814241513609886\n",
      "Epoch 150, Loss: 0.2792146746069193\n",
      "Epoch 200, Loss: 0.2478445852175355\n",
      "Epoch 250, Loss: 0.43327733781188726\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1)  # Sets the seed for PyTorch random number generators. This is important to ensure that the behavior of any operation that introduces randomness, such as the initialization of weights in neural networks and the shuffling of data in DataLoaders, is consistent across runs. Using a fixed seed value like 42 ensures that every execution of this code will produce the same results given the same inputs and model configuration. This is essential for debugging and comparing the performance impacts of any changes in a controlled manner.\n",
    "\n",
    "# Text preprocessing\n",
    "text = \"Tell me and I forget, teach me and I may remember, involve me and I learn\"\n",
    "words = text.lower().replace(',', '').split()\n",
    "vocab = {word: i for i, word in enumerate(set(words))}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Prepare data for the model\n",
    "inputs = []\n",
    "targets = []\n",
    "context_size = 3\n",
    "\n",
    "for i in range(len(words) - context_size):\n",
    "    input_idx = [vocab[words[j]] for j in range(i, i + context_size)]\n",
    "    target_idx = vocab[words[i + context_size]]\n",
    "    inputs.append(input_idx)\n",
    "    targets.append(target_idx)\n",
    "\n",
    "inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Define a multi-layer RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # RNN with multiple layers\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Input shape: (batch_size, sequence_length)\n",
    "        embeds = self.embeddings(inputs)  # (batch_size, sequence_length, embedding_dim)\n",
    "        rnn_out, _ = self.rnn(embeds)     # (batch_size, sequence_length, hidden_dim)\n",
    "        # We use the last RNN output as the representation of the sequence\n",
    "        final_output = rnn_out[:, -1, :] # (batch_size, hidden_dim)\n",
    "        out = self.fc(final_output)       # (batch_size, vocab_size)\n",
    "        return out\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "num_layers = 2  # Number of RNN layers\n",
    "\n",
    "# Initialize the RNN model with multiple layers, loss function, and optimizer\n",
    "model = SimpleRNN(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in dataloader:\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "# Prediction function\n",
    "def predict(text):\n",
    "    words = text.lower().replace(',', '').split()\n",
    "    input_idx = [vocab.get(word, 0) for word in words[-context_size:]]\n",
    "    input_tensor = torch.tensor([input_idx], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        log_probs = model(input_tensor)\n",
    "    return max(zip(log_probs[0].exp(), vocab.keys()), key=lambda p: p[0])[1]\n",
    "\n",
    "# Test the prediction function\n",
    "print(predict(\"teach me and\"))  # Expected to output 'I', given the training context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b03b5c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: 'teach me and'\n",
      "Word: \"i\", Score: 0.9907\n",
      "Word: \"involve\", Score: 0.0019\n",
      "Word: \"teach\", Score: 0.0015\n",
      "Word: \"me\", Score: 0.0014\n",
      "Word: \"tell\", Score: 0.0013\n",
      "Word: \"forget\", Score: 0.0011\n",
      "Word: \"may\", Score: 0.0009\n",
      "Word: \"remember\", Score: 0.0007\n",
      "Word: \"learn\", Score: 0.0003\n",
      "Word: \"and\", Score: 0.0001\n",
      "\n",
      "\n",
      "Context: 'and I may'\n",
      "Word: \"remember\", Score: 0.9600\n",
      "Word: \"and\", Score: 0.0152\n",
      "Word: \"teach\", Score: 0.0132\n",
      "Word: \"me\", Score: 0.0031\n",
      "Word: \"tell\", Score: 0.0025\n",
      "Word: \"forget\", Score: 0.0018\n",
      "Word: \"may\", Score: 0.0015\n",
      "Word: \"i\", Score: 0.0012\n",
      "Word: \"learn\", Score: 0.0008\n",
      "Word: \"involve\", Score: 0.0006\n",
      "\n",
      "\n",
      "Context: 'and I do'\n",
      "Word: \"remember\", Score: 0.9600\n",
      "Word: \"and\", Score: 0.0152\n",
      "Word: \"teach\", Score: 0.0132\n",
      "Word: \"me\", Score: 0.0031\n",
      "Word: \"tell\", Score: 0.0025\n",
      "Word: \"forget\", Score: 0.0018\n",
      "Word: \"may\", Score: 0.0015\n",
      "Word: \"i\", Score: 0.0012\n",
      "Word: \"learn\", Score: 0.0008\n",
      "Word: \"involve\", Score: 0.0006\n",
      "\n",
      "\n",
      "Context: 'tell me next'\n",
      "Word: \"i\", Score: 0.9802\n",
      "Word: \"forget\", Score: 0.0043\n",
      "Word: \"involve\", Score: 0.0040\n",
      "Word: \"may\", Score: 0.0030\n",
      "Word: \"tell\", Score: 0.0024\n",
      "Word: \"teach\", Score: 0.0016\n",
      "Word: \"remember\", Score: 0.0013\n",
      "Word: \"learn\", Score: 0.0013\n",
      "Word: \"me\", Score: 0.0012\n",
      "Word: \"and\", Score: 0.0007\n",
      "\n",
      "\n",
      "Context: 'do not tell'\n",
      "Word: \"and\", Score: 0.2678\n",
      "Word: \"involve\", Score: 0.2116\n",
      "Word: \"may\", Score: 0.1252\n",
      "Word: \"learn\", Score: 0.1157\n",
      "Word: \"forget\", Score: 0.0818\n",
      "Word: \"i\", Score: 0.0779\n",
      "Word: \"me\", Score: 0.0490\n",
      "Word: \"tell\", Score: 0.0298\n",
      "Word: \"teach\", Score: 0.0257\n",
      "Word: \"remember\", Score: 0.0154\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Improved Prediction Function\n",
    "def predict_all_scores(text):\n",
    "    \"\"\"\n",
    "    Predicts the probabilities of all possible next words based on the input text.\n",
    "    \n",
    "    Args:\n",
    "    text (str): A string of text from which the last few words are taken as context for prediction.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary of words and their associated probabilities, sorted by likelihood.\n",
    "    \"\"\"\n",
    "    # Normalize the input text: convert to lowercase and remove commas for consistent preprocessing\n",
    "    words = text.lower().replace(',', '').split()\n",
    "\n",
    "    # Convert the last few words to indices using the vocabulary.\n",
    "    # Use a default index (0) if a word is not found in the vocabulary.\n",
    "    input_idx = [vocab.get(word, 0) for word in words[-context_size:]]\n",
    "\n",
    "    # Create a tensor from the word indices, suitable for model input\n",
    "    input_tensor = torch.tensor([input_idx], dtype=torch.long)\n",
    "\n",
    "    # Disable gradient calculations for efficiency during inference\n",
    "    with torch.no_grad():\n",
    "        # Pass the tensor through the model to get log probabilities for each class (word)\n",
    "        log_probs = model(input_tensor)\n",
    "\n",
    "        # Apply softmax to convert log probabilities to actual probabilities\n",
    "        probs = torch.softmax(log_probs, dim=1)\n",
    "\n",
    "    # Convert the probabilities tensor to a list for easier processing\n",
    "    all_probs = probs.squeeze().tolist()\n",
    "\n",
    "    # Map each word in the vocabulary to its predicted probability\n",
    "    predicted_words_scores = {word: prob for word, idx in vocab.items() for prob in [all_probs[idx]]}\n",
    "\n",
    "    # Sort the predictions by their probabilities in descending order\n",
    "    predicted_words_scores = dict(sorted(predicted_words_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return predicted_words_scores\n",
    "\n",
    "# Test the updated prediction function with different contexts\n",
    "contexts = [\"teach me and\", \"and I may\", \"and I do\", \"tell me next\", \"do not tell\"]\n",
    "for context in contexts:\n",
    "    predicted_scores = predict_all_scores(context)\n",
    "    print(f\"Context: '{context}'\")\n",
    "    for word, score in predicted_scores.items():\n",
    "        print(f'Word: \"{word}\", Score: {score:.4f}')\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
